{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b64c8d5",
   "metadata": {},
   "source": [
    "# Project 1 Kaggle Competition - Pima Indians Diabetes Dataset\n",
    "\n",
    "Dhara Patel <br>\n",
    "300146860 <br>\n",
    "CSI 4106 - Fall 2022 <br>\n",
    "Group: 54\n",
    "\n",
    "### Understanding the dataset:\n",
    "This dataset is taken from the Kaggle Competition Pima Indians Diabetes Dataset published by UCI MACHINE LEARNING. It is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. There were several constraints on the selection of these instances from a larger database. In particular, all patients in this dataset are females who are at least 21 years old and of Pima Indian heritage.\n",
    "\n",
    "Seeing the rise in Diabetes diagonsis among many of my family members, I chose this dataset as I am really interested in knowing what is the probability of someone getting a diabetes. Although, this dataset is only of the Pima Indians, I am hoping to use these models to predict if someone has a diabetes with more data whenever it is published.\n",
    "\n",
    "This dataset is fairly small compared to other datasets, with less than 1000 datapoints. This dataset is fairly clean with not many values missing, making it a good choice for learning. It is a binary classification since the output column of the dataset is either 0 or 1, indicating 'No diabetes' or 'Diabetes', respectively. This is because the dataset predicts the outcome of the patient either having diabetes or not. \n",
    "When analyzing the data, one can see that there is no null value present in the dataset. However, there are some missing datapoints, which are set to 0. This could skew the results and training of the model. For instance, point at index 7 in the dataset is missing BloodPressure, SkinThickness and Insulin attributes, and hence are set to 0. There are 768 points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1815325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "5            5      116             74              0        0  25.6   \n",
       "6            3       78             50             32       88  31.0   \n",
       "7           10      115              0              0        0  35.3   \n",
       "8            2      197             70             45      543  30.5   \n",
       "9            8      125             96              0        0   0.0   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  \n",
       "5                     0.201   30        0  \n",
       "6                     0.248   26        1  \n",
       "7                     0.134   29        0  \n",
       "8                     0.158   53        1  \n",
       "9                     0.232   54        1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import unique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataFrame = pd.read_csv(\"/Users/dhara/Documents/uOttawa Fall sem 10/CSI 4106/Project1/diabetes.csv\")\n",
    "dataFrame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2d472e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/dhara/miniforge3/lib/python3.10/site-packages (3.6.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dhara/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e330981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /Users/dhara/miniforge3/lib/python3.10/site-packages (0.9.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b8484b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5a0cc",
   "metadata": {},
   "source": [
    "#### Brainstorming the attributes:\n",
    "It seems that all the necessary features are present for diagnosis of diabetes. However, there could be other symptoms added to the list of attributes in the future such as \"sensation\", since most patients with diabetes also feel there is a \"tingling sensation\" on some part of their body, which is related to diabetes. This could help with the prediction for a more accurate prediction. All the attributes in the dataset are useful, so none of them should be discarded when doing the training.\n",
    "\n",
    "#### Encoding the features:\n",
    "Since I used Gaussian Naive Bayes, it assumes that the data is continuous, the data was left as it is since it is continuous data. As for Logistic Regression, since it works best with continuous data, there was not a need to encode the data to discrete attributes. In addition, Multi-Layer Perceptron works with continuous data as well.\n",
    "\n",
    "### Preparing the data for 4-fold cross-validation\n",
    "Here, variable x contains the data of all the attributes and y contains the data for the output, which is labelled \"Outcome\" in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a0d6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataFrame.drop(\"Outcome\", axis=1)\n",
    "# predictions\n",
    "y = dataFrame[\"Outcome\"]\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=4)\n",
    "report_ListGB = [] # 2D array to store accuracy, precision, recall for Naive Bayes\n",
    "report_ListLR = [] # For logistic regression\n",
    "report_ListMLP = [] # For MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d487f39",
   "metadata": {},
   "source": [
    "#### Step 5-7: 4-fold cross-validation using cross_validate function for GB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "19e96733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of each cross-validation section: [0.77083333 0.71354167 0.75520833 0.765625  ]\n",
      "Precision [0.69491525 0.6        0.66666667 0.67741935]\n",
      "Recall [0.6119403  0.53731343 0.59701493 0.62686567]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall']\n",
    "gasNB = GaussianNB()\n",
    "scores = cross_validate(gasNB, x, y, scoring=scoring, cv=4, return_estimator=True, return_train_score=True)\n",
    "print(\"Test accuracy of each cross-validation section:\", scores['test_accuracy'])\n",
    "print(\"Precision\", scores['test_precision'])\n",
    "print(\"Recall\", scores['test_recall'])\n",
    "gsNB = gasNB.fit(x, y)\n",
    "y_pred = gsNB.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa8882b",
   "metadata": {},
   "source": [
    "### Step 5: Training 3 models using some default parameters. \n",
    "a. Na√Øve Bayes - Gaussian Naive Bayes <br>\n",
    "b. Logistic Regression<br>\n",
    "c. Multi-Layer Perceptron<br>\n",
    " <br>\n",
    "Here I am using Gaussian Naive Bayes since it is relatively fast to train and use and it is highly scalable. In addition, I am using a single hidden layer for MLP classification since it is better to start with a simpler model than overfitting or complicating the model unnecessarily. <br>\n",
    "\n",
    "Doing the cross-validation manually. In this case, using kfold and 4 splits, 4-fold cross validation is used. However, instead of keeping a test case separately. 4 models were trained on all data. Within each validation, the 75% of the data was used to train, and 25% of the data was used to test the model. Then, the average of all these 4 models was taken to evaluate the accuracy, precision and recall. <br>\n",
    " <br>\n",
    "Using the for loop to iterate through each section for cross validation and storing the data in the report_List variables for keeping track of accuracy, precision and recall for each validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de660f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussisan Naive Bayes\n",
      "Confusion matrix: TP = 103 , TN = 42 , FP = 23 , FN = 24\n",
      "The accuracy score test:  0.7552083333333334\n",
      "The precision score is:  0.6461538461538462\n",
      "The recall score is:  0.6363636363636364\n",
      "Classification report below: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.81      0.82      0.81       126\n",
      "    Diabetes       0.65      0.64      0.64        66\n",
      "\n",
      "    accuracy                           0.76       192\n",
      "   macro avg       0.73      0.73      0.73       192\n",
      "weighted avg       0.75      0.76      0.75       192\n",
      "\n",
      "Logistic Regression analysis\n",
      "\n",
      "Confusion matrix: TP = 110 , TN = 44 , FP = 16 , FN = 22\n",
      "The accuracy score is:  0.8020833333333334\n",
      "The precision score is:  0.7333333333333333\n",
      "The recall score is:  0.6666666666666666\n",
      "Classification report below: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.83      0.87      0.85       126\n",
      "    Diabetes       0.73      0.67      0.70        66\n",
      "\n",
      "    accuracy                           0.80       192\n",
      "   macro avg       0.78      0.77      0.78       192\n",
      "weighted avg       0.80      0.80      0.80       192\n",
      "\n",
      "Multi-layer Perceptron analysis\n",
      "\n",
      "Confusion matrix of MLP: TP = 113 , TN = 34 , FP = 13 , FN = 32\n",
      "The accuracy score is:  0.765625\n",
      "The precision score is:  0.723404255319149\n",
      "The recall score is:  0.5151515151515151\n",
      "Classification report below 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.78      0.90      0.83       126\n",
      "    Diabetes       0.72      0.52      0.60        66\n",
      "\n",
      "    accuracy                           0.77       192\n",
      "   macro avg       0.75      0.71      0.72       192\n",
      "weighted avg       0.76      0.77      0.75       192\n",
      "\n",
      "Gaussisan Naive Bayes\n",
      "Confusion matrix: TP = 112 , TN = 37 , FP = 18 , FN = 25\n",
      "The accuracy score test:  0.7760416666666666\n",
      "The precision score is:  0.6727272727272727\n",
      "The recall score is:  0.5967741935483871\n",
      "Classification report below: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.82      0.86      0.84       130\n",
      "    Diabetes       0.67      0.60      0.63        62\n",
      "\n",
      "    accuracy                           0.78       192\n",
      "   macro avg       0.75      0.73      0.74       192\n",
      "weighted avg       0.77      0.78      0.77       192\n",
      "\n",
      "Logistic Regression analysis\n",
      "\n",
      "Confusion matrix: TP = 122 , TN = 35 , FP = 8 , FN = 27\n",
      "The accuracy score is:  0.8177083333333334\n",
      "The precision score is:  0.813953488372093\n",
      "The recall score is:  0.5645161290322581\n",
      "Classification report below: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.82      0.94      0.87       130\n",
      "    Diabetes       0.81      0.56      0.67        62\n",
      "\n",
      "    accuracy                           0.82       192\n",
      "   macro avg       0.82      0.75      0.77       192\n",
      "weighted avg       0.82      0.82      0.81       192\n",
      "\n",
      "Multi-layer Perceptron analysis\n",
      "\n",
      "Confusion matrix of MLP: TP = 117 , TN = 26 , FP = 13 , FN = 36\n",
      "The accuracy score is:  0.7447916666666666\n",
      "The precision score is:  0.6666666666666666\n",
      "The recall score is:  0.41935483870967744\n",
      "Classification report below 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.76      0.90      0.83       130\n",
      "    Diabetes       0.67      0.42      0.51        62\n",
      "\n",
      "    accuracy                           0.74       192\n",
      "   macro avg       0.72      0.66      0.67       192\n",
      "weighted avg       0.73      0.74      0.73       192\n",
      "\n",
      "Gaussisan Naive Bayes\n",
      "Confusion matrix: TP = 100 , TN = 40 , FP = 22 , FN = 30\n",
      "The accuracy score test:  0.7291666666666666\n",
      "The precision score is:  0.6451612903225806\n",
      "The recall score is:  0.5714285714285714\n",
      "Classification report below: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.77      0.82      0.79       122\n",
      "    Diabetes       0.65      0.57      0.61        70\n",
      "\n",
      "    accuracy                           0.73       192\n",
      "   macro avg       0.71      0.70      0.70       192\n",
      "weighted avg       0.72      0.73      0.73       192\n",
      "\n",
      "Logistic Regression analysis\n",
      "\n",
      "Confusion matrix: TP = 103 , TN = 36 , FP = 19 , FN = 34\n",
      "The accuracy score is:  0.7239583333333334\n",
      "The precision score is:  0.6545454545454545\n",
      "The recall score is:  0.5142857142857142\n",
      "Classification report below: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.75      0.84      0.80       122\n",
      "    Diabetes       0.65      0.51      0.58        70\n",
      "\n",
      "    accuracy                           0.72       192\n",
      "   macro avg       0.70      0.68      0.69       192\n",
      "weighted avg       0.72      0.72      0.72       192\n",
      "\n",
      "Multi-layer Perceptron analysis\n",
      "\n",
      "Confusion matrix of MLP: TP = 105 , TN = 22 , FP = 17 , FN = 48\n",
      "The accuracy score is:  0.6614583333333334\n",
      "The precision score is:  0.5641025641025641\n",
      "The recall score is:  0.3142857142857143\n",
      "Classification report below 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.69      0.86      0.76       122\n",
      "    Diabetes       0.56      0.31      0.40        70\n",
      "\n",
      "    accuracy                           0.66       192\n",
      "   macro avg       0.63      0.59      0.58       192\n",
      "weighted avg       0.64      0.66      0.63       192\n",
      "\n",
      "Gaussisan Naive Bayes\n",
      "Confusion matrix: TP = 104 , TN = 37 , FP = 18 , FN = 33\n",
      "The accuracy score test:  0.734375\n",
      "The precision score is:  0.6727272727272727\n",
      "The recall score is:  0.5285714285714286\n",
      "Classification report below: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.76      0.85      0.80       122\n",
      "    Diabetes       0.67      0.53      0.59        70\n",
      "\n",
      "    accuracy                           0.73       192\n",
      "   macro avg       0.72      0.69      0.70       192\n",
      "weighted avg       0.73      0.73      0.73       192\n",
      "\n",
      "Logistic Regression analysis\n",
      "\n",
      "Confusion matrix: TP = 107 , TN = 37 , FP = 15 , FN = 33\n",
      "The accuracy score is:  0.75\n",
      "The precision score is:  0.7115384615384616\n",
      "The recall score is:  0.5285714285714286\n",
      "Classification report below: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.76      0.88      0.82       122\n",
      "    Diabetes       0.71      0.53      0.61        70\n",
      "\n",
      "    accuracy                           0.75       192\n",
      "   macro avg       0.74      0.70      0.71       192\n",
      "weighted avg       0.75      0.75      0.74       192\n",
      "\n",
      "Multi-layer Perceptron analysis\n",
      "\n",
      "Confusion matrix of MLP: TP = 98 , TN = 36 , FP = 24 , FN = 34\n",
      "The accuracy score is:  0.6979166666666666\n",
      "The precision score is:  0.6\n",
      "The recall score is:  0.5142857142857142\n",
      "Classification report below 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No diabetes       0.74      0.80      0.77       122\n",
      "    Diabetes       0.60      0.51      0.55        70\n",
      "\n",
      "    accuracy                           0.70       192\n",
      "   macro avg       0.67      0.66      0.66       192\n",
      "weighted avg       0.69      0.70      0.69       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0 # Used to keep count of the cross-validation section\n",
    "for train_index, test_index in kf.split(x):\n",
    "\n",
    "    # Training and testing data set for each fold\n",
    "    x_training, x_testing = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_training, y_testing = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "\n",
    "    # Step 6-7 Training and Testing Gaussian Naive Bayes\n",
    "    print(\"Gaussisan Naive Bayes\")\n",
    "    gaussNB = GaussianNB().fit(x_training, y_training)\n",
    "\n",
    "    # Predicting the outcome\n",
    "    y_prediction_GB = gaussNB.predict(x_testing)\n",
    "\n",
    "    # confusion matrix\n",
    "    conmGB = confusion_matrix(y_testing, y_prediction_GB)\n",
    "    print(\"Confusion matrix: TP =\", conmGB[0,0], \", TN =\", conmGB[1,1], \", FP =\", conmGB[0,1], \", FN =\", conmGB[1,0])\n",
    "    print(\"The accuracy score test: \", accuracy_score(y_testing, y_prediction_GB))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_GB))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_GB))\n",
    "    cl_report_GB =classification_report(y_testing, y_prediction_GB, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    print(\"Classification report below:\", count)\n",
    "    print(cl_report_GB)\n",
    "    class_report_GB = [accuracy_score(y_testing, y_prediction_GB), precision_score(y_testing, y_prediction_GB), recall_score(y_testing, y_prediction_GB)]\n",
    "    report_ListGB.append(class_report_GB)\n",
    "\n",
    "\n",
    "    # Step 6-7 Training and testing Logistic Regression\n",
    "    print(\"Logistic Regression analysis\\n\")\n",
    "    logistic_reg = LogisticRegression(penalty='l2', fit_intercept = True, max_iter=1000).fit(x_training, y_training)\n",
    "    y_prediction_LR = logistic_reg.predict(x_testing)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conmLR = confusion_matrix(y_testing, y_prediction_LR)\n",
    "    print(\"Confusion matrix: TP =\", conmLR[0,0], \", TN =\", conmLR[1,1], \", FP =\", conmLR[0,1], \", FN =\", conmLR[1,0])\n",
    "    print(\"The accuracy score is: \", accuracy_score(y_testing, y_prediction_LR))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_LR))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_LR))\n",
    "\n",
    "    cl_report_LR = classification_report(y_testing, y_prediction_LR, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    print(\"Classification report below:\", count)\n",
    "    print(cl_report_LR)\n",
    "    class_report_LR = [accuracy_score(y_testing, y_prediction_LR), precision_score(y_testing, y_prediction_LR), recall_score(y_testing, y_prediction_LR)]\n",
    "    report_ListLR.append(class_report_LR)\n",
    "\n",
    "\n",
    "    # MLP classification with 1 hidden layer \n",
    "    # Step 6-7: Testing and training\n",
    "    print(\"Multi-layer Perceptron analysis\\n\")\n",
    "\n",
    "    mlp = MLPClassifier(solver=\"adam\", max_iter=1000, activation=\"relu\", hidden_layer_sizes=(8), alpha=0.1, batch_size='auto', learning_rate_init=0.001, random_state=2)\n",
    "    mlp.fit(x_training, y_training)\n",
    "\n",
    "    y_prediction_MLP = mlp.predict(x_testing)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conmMLP = confusion_matrix(y_testing, y_prediction_MLP)\n",
    "    print(\"Confusion matrix of MLP: TP =\", conmMLP[0,0], \", TN =\", conmMLP[1,1], \", FP =\", conmMLP[0,1], \", FN =\", conmMLP[1,0])\n",
    "    print(\"The accuracy score is: \", accuracy_score(y_testing, y_prediction_MLP))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_MLP))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_MLP))\n",
    "\n",
    "    cl_report_MLP = classification_report(y_testing, y_prediction_MLP, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    print(\"Classification report below\", count)\n",
    "    print(cl_report_MLP)\n",
    "    class_report_MLP = [accuracy_score(y_testing, y_prediction_MLP), precision_score(y_testing, y_prediction_MLP), recall_score(y_testing, y_prediction_MLP)]\n",
    "    report_ListMLP.append(class_report_MLP)\n",
    "\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e9b21f",
   "metadata": {},
   "source": [
    "### Step 7-8: Evaluation\n",
    "Getting the average of accuracy, precision, recall of cross validation of the first 4 models of each type created in Step 6-7 for cross validation. This gives the overall score and gets the average by having each model train and test on different part of the dataset to avoid bias of the position of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c44dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, Recall of normal Gaussian Naive-Bayes [0.74869792 0.65919242 0.58328446]\n",
      "Accuracy, Precision, Recall of normal Logistic Regression [0.7734375  0.72834268 0.56850998]\n",
      "Accuracy, Precision, Recall of normal Multi-layer Perceptron [0.71744792 0.63854337 0.44076945]\n"
     ]
    }
   ],
   "source": [
    "averaged_reportGB = np.array(report_ListGB).mean(axis=0)\n",
    "print(\"Accuracy, Precision, Recall of normal Gaussian Naive-Bayes\", averaged_reportGB)\n",
    "\n",
    "averaged_reportLR = np.array(report_ListLR).mean(axis=0)\n",
    "print(\"Accuracy, Precision, Recall of normal Logistic Regression\", averaged_reportLR)\n",
    "\n",
    "averaged_reportMLP = np.array(report_ListMLP).mean(axis=0)\n",
    "print(\"Accuracy, Precision, Recall of normal Multi-layer Perceptron\", averaged_reportMLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc064107",
   "metadata": {},
   "source": [
    "### Step 9: Modifying some parameters, and performing a train/test/evaluate again:\n",
    "The cross-validation performed here is the same as the cross-validation performed earlier. Using the 4-fold cross validation to split the dataset into 4, and iterating through each set and training and testing on the remaining set.\n",
    "\n",
    "#### Gaussian Naive Bayes Model 2:\n",
    "The parameter changed for Gaussian Naive Bayes was Prior by calculating the prior probability. This allows for a better prediction, knowing the prior probability and using it when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "55ac5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter prior added to normal GB\n"
     ]
    }
   ],
   "source": [
    "# Parameters changed for GB: Prior\n",
    "print(\"Parameter prior added to normal GB\")\n",
    "count = 0\n",
    "unique_y = y.unique()\n",
    "prior_probability = np.zeros(len(y.unique()))\n",
    "for i in range(0,len(unique_y)):\n",
    "    prior_probability[i]=sum(dataFrame[\"Outcome\"]==unique_y[i])/len(dataFrame[\"Outcome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d9fc9",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model 2:\n",
    "The parameter changed for Logistic Regression was the class_weight. It is now set to be \"balanced\", which adjusts the weights inversely proportional to class frequencies.\n",
    "\n",
    "#### Multi-Layer Perceptron Model 2:\n",
    "The parameter changed for MLP was the solver type, which I set to 'lbfgs'. This is recommended for a smaller dataset and my data is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777b770",
   "metadata": {},
   "source": [
    "##### Cross-validation again:\n",
    "Using the same cross-validation as earlier, using the same naming convention for variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "13034e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 1 0 0 1 0 1]\n",
      "Confusion matrix of GB2: TP = 86 , TN = 52 , FP = 40 , FN = 14\n",
      "The accuracy score test:  0.71875\n",
      "The precision score is:  0.5652173913043478\n",
      "The recall score is:  0.7878787878787878\n",
      "Logistic Regression analysis with modified parameter class_weight: 'balanced'\n",
      "\n",
      "Confusion matrix of LR2: TP = 95 , TN = 51 , FP = 31 , FN = 15\n",
      "The accuracy score is:  0.7604166666666666\n",
      "The precision score is:  0.6219512195121951\n",
      "The recall score is:  0.7727272727272727\n",
      "Multi-layer Perceptron analysis - modified solver to 'lbfgs'\n",
      "\n",
      "Confusion matrix of MLP2: TP = 109 , TN = 37 , FP = 17 , FN = 29\n",
      "The accuracy score is:  0.7604166666666666\n",
      "The precision score is:  0.6851851851851852\n",
      "The recall score is:  0.5606060606060606\n",
      "[0 0 1 0 1 1 0 1 1 1]\n",
      "Confusion matrix of GB2: TP = 95 , TN = 47 , FP = 35 , FN = 15\n",
      "The accuracy score test:  0.7395833333333334\n",
      "The precision score is:  0.573170731707317\n",
      "The recall score is:  0.7580645161290323\n",
      "Logistic Regression analysis with modified parameter class_weight: 'balanced'\n",
      "\n",
      "Confusion matrix of LR2: TP = 105 , TN = 45 , FP = 25 , FN = 17\n",
      "The accuracy score is:  0.78125\n",
      "The precision score is:  0.6428571428571429\n",
      "The recall score is:  0.7258064516129032\n",
      "Multi-layer Perceptron analysis - modified solver to 'lbfgs'\n",
      "\n",
      "Confusion matrix of MLP2: TP = 115 , TN = 35 , FP = 15 , FN = 27\n",
      "The accuracy score is:  0.78125\n",
      "The precision score is:  0.7\n",
      "The recall score is:  0.5645161290322581\n",
      "[1 0 1 1 1 1 1 1 0 1]\n",
      "Confusion matrix of GB2: TP = 80 , TN = 51 , FP = 42 , FN = 19\n",
      "The accuracy score test:  0.6822916666666666\n",
      "The precision score is:  0.5483870967741935\n",
      "The recall score is:  0.7285714285714285\n",
      "Logistic Regression analysis with modified parameter class_weight: 'balanced'\n",
      "\n",
      "Confusion matrix of LR2: TP = 86 , TN = 54 , FP = 36 , FN = 16\n",
      "The accuracy score is:  0.7291666666666666\n",
      "The precision score is:  0.6\n",
      "The recall score is:  0.7714285714285715\n",
      "Multi-layer Perceptron analysis - modified solver to 'lbfgs'\n",
      "\n",
      "Confusion matrix of MLP2: TP = 102 , TN = 24 , FP = 20 , FN = 46\n",
      "The accuracy score is:  0.65625\n",
      "The precision score is:  0.5454545454545454\n",
      "The recall score is:  0.34285714285714286\n",
      "[1 1 0 1 0 0 1 0 1 0]\n",
      "Confusion matrix of GB2: TP = 91 , TN = 50 , FP = 31 , FN = 20\n",
      "The accuracy score test:  0.734375\n",
      "The precision score is:  0.6172839506172839\n",
      "The recall score is:  0.7142857142857143\n",
      "Logistic Regression analysis with modified parameter class_weight: 'balanced'\n",
      "\n",
      "Confusion matrix of LR2: TP = 97 , TN = 45 , FP = 25 , FN = 25\n",
      "The accuracy score is:  0.7395833333333334\n",
      "The precision score is:  0.6428571428571429\n",
      "The recall score is:  0.6428571428571429\n",
      "Multi-layer Perceptron analysis - modified solver to 'lbfgs'\n",
      "\n",
      "Confusion matrix of MLP2: TP = 98 , TN = 36 , FP = 24 , FN = 34\n",
      "The accuracy score is:  0.6979166666666666\n",
      "The precision score is:  0.6\n",
      "The recall score is:  0.5142857142857142\n"
     ]
    }
   ],
   "source": [
    "report_ListGB2 = [] # 2D array to store accuracy, precision, recall\n",
    "report_ListLR2 = []\n",
    "report_ListMLP2 = []\n",
    "\n",
    "# Classification report after parameter changes - Redoing the cross-validation after adding prior probability\n",
    "for train_index, test_index in kf.split(x):\n",
    "    x_training, x_testing = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_training, y_testing = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Cross validation for GB after adding prior parameter\n",
    "    gaussNB2 = GaussianNB(priors=prior_probability).fit(x_training, y_training)\n",
    "\n",
    "    # Predicting the outcome\n",
    "    print(gaussNB2.predict(x_testing)[:10])\n",
    "\n",
    "    y_prediction_GB2 = gaussNB2.predict(x_testing)\n",
    "\n",
    "    # confusion matrix\n",
    "    conmGB2 = confusion_matrix(y_testing, y_prediction_GB2)\n",
    "    print(\"Confusion matrix of GB2: TP =\", conmGB2[0,0], \", TN =\", conmGB2[1,1], \", FP =\", conmGB2[0,1], \", FN =\", conmGB2[1,0])\n",
    "    print(\"The accuracy score test: \", accuracy_score(y_testing, y_prediction_GB2))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_GB2))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_GB2))\n",
    "    cl_report_GB2 =classification_report(y_testing, y_prediction_GB2)\n",
    "    #print(\"Classification report below:\", count)\n",
    "    #print(cl_report_GB2)\n",
    "    class_report_GB2 = [accuracy_score(y_testing, y_prediction_GB2), precision_score(y_testing, y_prediction_GB2), recall_score(y_testing, y_prediction_GB2)]\n",
    "    report_ListGB2.append(class_report_GB2)\n",
    "    \n",
    "    # Model 2 for LR\n",
    "    # Training and testing LogisticRegression with modified class_weight parameter to be balanced,\n",
    "    # adjust the weights inversely proportional to class frequencies\n",
    "    print(\"Logistic Regression analysis with modified parameter class_weight: 'balanced'\\n\")\n",
    "    logistic_reg2 = LogisticRegression(penalty='l2', fit_intercept = True, max_iter=1000, class_weight='balanced').fit(x_training, y_training)\n",
    "    y_prediction_LR2 = logistic_reg2.predict(x_testing)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conmLR2 = confusion_matrix(y_testing, y_prediction_LR2)\n",
    "    print(\"Confusion matrix of LR2: TP =\", conmLR2[0,0], \", TN =\", conmLR2[1,1], \", FP =\", conmLR2[0,1], \", FN =\", conmLR2[1,0])\n",
    "    print(\"The accuracy score is: \", accuracy_score(y_testing, y_prediction_LR2))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_LR2))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_LR2))\n",
    "\n",
    "    cl_report_LR2 = classification_report(y_testing, y_prediction_LR2, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    #print(\"Classification report below:\", count)\n",
    "    #print(cl_report_LR2)\n",
    "    class_report_LR2 = [accuracy_score(y_testing, y_prediction_LR2), precision_score(y_testing, y_prediction_LR2), recall_score(y_testing, y_prediction_LR2)]\n",
    "    report_ListLR2.append(class_report_LR2)\n",
    "\n",
    "\n",
    "    # Model 2 for MLP classification with 1 hidden layer, Changing parameter type solver to lbggs since I have a small dataset, a little less than 1k\n",
    "    print(\"Multi-layer Perceptron analysis - modified solver to 'lbfgs'\\n\")\n",
    "    mlp2 = MLPClassifier(solver=\"lbfgs\", max_iter=3000, activation=\"relu\", hidden_layer_sizes=(8), alpha=0.0001, learning_rate_init=0.001, random_state=2)\n",
    "    mlp2.fit(x_training, y_training)\n",
    "\n",
    "    y_prediction_MLP2 = mlp.predict(x_testing)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conmMLP2 = confusion_matrix(y_testing, y_prediction_MLP2)\n",
    "    print(\"Confusion matrix of MLP2: TP =\", conmMLP2[0,0], \", TN =\", conmMLP2[1,1], \", FP =\", conmMLP2[0,1], \", FN =\", conmMLP2[1,0])\n",
    "    print(\"The accuracy score is: \", accuracy_score(y_testing, y_prediction_MLP2))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_MLP2))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_MLP2))\n",
    "\n",
    "    cl_report_MLP2 = classification_report(y_testing, y_prediction_MLP2, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    \n",
    "    #print(\"Classification report below\", count)\n",
    "    #print(cl_report_MLP2)\n",
    "    class_report_MLP2 = [accuracy_score(y_testing, y_prediction_MLP2), precision_score(y_testing, y_prediction_MLP2), recall_score(y_testing, y_prediction_MLP2)]\n",
    "    report_ListMLP2.append(class_report_MLP2)\n",
    "\n",
    "\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6db4c",
   "metadata": {},
   "source": [
    "##### Average report of each model after the modification and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fe1c3e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy, precision, recall after cross validation of 2nd GB model: [0.71875    0.57601479 0.74720011]\n",
      "Average accuracy, precision, recall after cross validation of 2nd LR model: [0.75260417 0.62691638 0.72820486]\n",
      "Average accuray, precision, recall after cross validation of 2nd MLP model: [0.72395833 0.63265993 0.49556626]\n"
     ]
    }
   ],
   "source": [
    "averaged_reportGB2 = np.array(report_ListGB2).mean(axis=0)\n",
    "print(\"Average accuracy, precision, recall after cross validation of 2nd GB model:\", averaged_reportGB2)\n",
    "\n",
    "averaged_reportLR2 = np.array(report_ListLR2).mean(axis=0)\n",
    "print(\"Average accuracy, precision, recall after cross validation of 2nd LR model:\", averaged_reportLR2)\n",
    "\n",
    "averaged_reportMLP2 = np.array(report_ListMLP2).mean(axis=0)\n",
    "print(\"Average accuray, precision, recall after cross validation of 2nd MLP model:\", averaged_reportMLP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3dfe7a",
   "metadata": {},
   "source": [
    "### Repeating Step 9 for Modified parameters:\n",
    "\n",
    "#### Gaussian Naive Bayes Model 3:\n",
    "The parameter changed here was smoothing variance. This is because for Gaussian Naive Bayes only prior could be changed by calculating the prior probability. However, for variance smoothing, I am estimating a constant to apply the smoothing.\n",
    "\n",
    "#### Logistic Regression Model 3:\n",
    "The parameter changed for Logistic Regression was the C, which is the inverse of regularization. It is now set to 1e12, which will allow for better prediction on test set by reducing overfitting on the training set.\n",
    "\n",
    "#### Multi-Layer Perceptron Model 3:\n",
    "The parameter changed for MLP was increasing the hidden layer nodes to 13 and setting the activation parameter to \"identity\". This is to implement a linear bottleneck and increasing nodes is helpful in this case to provide better prediction.\n",
    "\n",
    "##### Cross-validation again:\n",
    "Using the same cross-validation as earlier, using the same naming convention for variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "309e4dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian NB analysis with modified parameter var_smoothing=0.01\n",
      "Confusion matrix of GB3: TP = 116 , TN = 38 , FP = 10 , FN = 28\n",
      "The accuracy score test:  0.8020833333333334\n",
      "The precision score is:  0.7916666666666666\n",
      "The recall score is:  0.5757575757575758\n",
      "\n",
      "Logistic Regression analysis with modified parameter C: 1e12\n",
      "\n",
      "Confusion matrix: TP = 110 , TN = 44 , FP = 16 , FN = 22\n",
      "The accuracy score is:  0.8020833333333334\n",
      "The precision score is:  0.7333333333333333\n",
      "The recall score is:  0.6666666666666666\n",
      "\n",
      "Multi-layer Perceptron analysis - modified activation to identity (implementing linear bottleneck) - increased # of nodes\n",
      "\n",
      "Confusion matrix of MLP3: TP = 111 , TN = 44 , FP = 15 , FN = 22\n",
      "The accuracy score is:  0.8072916666666666\n",
      "The precision score is:  0.7457627118644068\n",
      "The recall score is:  0.6666666666666666\n",
      "The last two columns are: Actual Diagnosis, Predicted Diagnosis\n",
      "FP/FN for GB\n",
      " [[2.000e+00 1.970e+02 7.000e+01 4.500e+01 5.430e+02 3.050e+01 1.580e-01\n",
      "  5.300e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+01 1.680e+02 7.400e+01 0.000e+00 0.000e+00 3.800e+01 5.370e-01\n",
      "  3.400e+01 1.000e+00 1.000e+00]\n",
      " [7.000e+00 1.070e+02 7.400e+01 0.000e+00 0.000e+00 2.960e+01 2.540e-01\n",
      "  3.100e+01 1.000e+00 0.000e+00]\n",
      " [1.000e+00 1.030e+02 3.000e+01 3.800e+01 8.300e+01 4.330e+01 1.830e-01\n",
      "  3.300e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.580e+02 7.600e+01 3.600e+01 2.450e+02 3.160e+01 8.510e-01\n",
      "  2.800e+01 1.000e+00 1.000e+00]\n",
      " [6.000e+00 9.200e+01 9.200e+01 0.000e+00 0.000e+00 1.990e+01 1.880e-01\n",
      "  2.800e+01 0.000e+00 0.000e+00]\n",
      " [7.000e+00 1.060e+02 9.200e+01 1.800e+01 0.000e+00 2.270e+01 2.350e-01\n",
      "  4.800e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.800e+02 6.600e+01 3.900e+01 0.000e+00 4.200e+01 1.893e+00\n",
      "  2.500e+01 1.000e+00 1.000e+00]\n",
      " [2.000e+00 7.100e+01 7.000e+01 2.700e+01 0.000e+00 2.800e+01 5.860e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [8.000e+00 1.760e+02 9.000e+01 3.400e+01 3.000e+02 3.370e+01 4.670e-01\n",
      "  5.800e+01 1.000e+00 1.000e+00]\n",
      " [5.000e+00 4.400e+01 6.200e+01 0.000e+00 0.000e+00 2.500e+01 5.870e-01\n",
      "  3.600e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.410e+02 5.800e+01 3.400e+01 1.280e+02 2.540e+01 6.990e-01\n",
      "  2.400e+01 0.000e+00 0.000e+00]\n",
      " [5.000e+00 9.900e+01 7.400e+01 2.700e+01 0.000e+00 2.900e+01 2.030e-01\n",
      "  3.200e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.090e+02 8.800e+01 3.000e+01 0.000e+00 3.250e+01 8.550e-01\n",
      "  3.800e+01 1.000e+00 0.000e+00]\n",
      " [2.000e+00 1.090e+02 9.200e+01 0.000e+00 0.000e+00 4.270e+01 8.450e-01\n",
      "  5.400e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.000e+02 6.600e+01 2.000e+01 9.000e+01 3.290e+01 8.670e-01\n",
      "  2.800e+01 1.000e+00 0.000e+00]\n",
      " [1.300e+01 1.260e+02 9.000e+01 0.000e+00 0.000e+00 4.340e+01 5.830e-01\n",
      "  4.200e+01 1.000e+00 0.000e+00]\n",
      " [1.000e+00 7.900e+01 7.500e+01 3.000e+01 0.000e+00 3.200e+01 3.960e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [7.000e+00 6.200e+01 7.800e+01 0.000e+00 0.000e+00 3.260e+01 3.910e-01\n",
      "  4.100e+01 0.000e+00 0.000e+00]\n",
      " [5.000e+00 9.500e+01 7.200e+01 3.300e+01 0.000e+00 3.770e+01 3.700e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.130e+02 4.400e+01 1.300e+01 0.000e+00 2.240e+01 1.400e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.010e+02 6.500e+01 2.800e+01 0.000e+00 2.460e+01 2.370e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 1.070e+02 6.800e+01 1.900e+01 0.000e+00 2.650e+01 1.650e-01\n",
      "  2.400e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 8.000e+01 5.500e+01 0.000e+00 0.000e+00 1.910e+01 2.580e-01\n",
      "  2.100e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 8.500e+01 6.500e+01 0.000e+00 0.000e+00 3.960e+01 9.300e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 9.600e+01 1.220e+02 0.000e+00 0.000e+00 2.240e+01 2.070e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 8.300e+01 5.800e+01 3.100e+01 1.800e+01 3.430e+01 3.360e-01\n",
      "  2.500e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.710e+02 7.200e+01 3.300e+01 1.350e+02 3.330e+01 1.990e-01\n",
      "  2.400e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+00 8.900e+01 7.600e+01 3.400e+01 3.700e+01 3.120e+01 1.920e-01\n",
      "  2.300e+01 0.000e+00 0.000e+00]\n",
      " [4.000e+00 7.600e+01 6.200e+01 0.000e+00 0.000e+00 3.400e+01 3.910e-01\n",
      "  2.500e+01 0.000e+00 0.000e+00]] \n",
      "\n",
      "FP/FN for LR\n",
      " [[2.000e+00 1.970e+02 7.000e+01 4.500e+01 5.430e+02 3.050e+01 1.580e-01\n",
      "  5.300e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+01 1.680e+02 7.400e+01 0.000e+00 0.000e+00 3.800e+01 5.370e-01\n",
      "  3.400e+01 1.000e+00 1.000e+00]\n",
      " [7.000e+00 1.070e+02 7.400e+01 0.000e+00 0.000e+00 2.960e+01 2.540e-01\n",
      "  3.100e+01 1.000e+00 0.000e+00]\n",
      " [1.000e+00 1.030e+02 3.000e+01 3.800e+01 8.300e+01 4.330e+01 1.830e-01\n",
      "  3.300e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.580e+02 7.600e+01 3.600e+01 2.450e+02 3.160e+01 8.510e-01\n",
      "  2.800e+01 1.000e+00 1.000e+00]\n",
      " [6.000e+00 9.200e+01 9.200e+01 0.000e+00 0.000e+00 1.990e+01 1.880e-01\n",
      "  2.800e+01 0.000e+00 0.000e+00]\n",
      " [7.000e+00 1.060e+02 9.200e+01 1.800e+01 0.000e+00 2.270e+01 2.350e-01\n",
      "  4.800e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.800e+02 6.600e+01 3.900e+01 0.000e+00 4.200e+01 1.893e+00\n",
      "  2.500e+01 1.000e+00 1.000e+00]\n",
      " [2.000e+00 7.100e+01 7.000e+01 2.700e+01 0.000e+00 2.800e+01 5.860e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [8.000e+00 1.760e+02 9.000e+01 3.400e+01 3.000e+02 3.370e+01 4.670e-01\n",
      "  5.800e+01 1.000e+00 1.000e+00]\n",
      " [5.000e+00 4.400e+01 6.200e+01 0.000e+00 0.000e+00 2.500e+01 5.870e-01\n",
      "  3.600e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.410e+02 5.800e+01 3.400e+01 1.280e+02 2.540e+01 6.990e-01\n",
      "  2.400e+01 0.000e+00 0.000e+00]\n",
      " [5.000e+00 9.900e+01 7.400e+01 2.700e+01 0.000e+00 2.900e+01 2.030e-01\n",
      "  3.200e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.090e+02 8.800e+01 3.000e+01 0.000e+00 3.250e+01 8.550e-01\n",
      "  3.800e+01 1.000e+00 0.000e+00]\n",
      " [2.000e+00 1.090e+02 9.200e+01 0.000e+00 0.000e+00 4.270e+01 8.450e-01\n",
      "  5.400e+01 0.000e+00 1.000e+00]\n",
      " [2.000e+00 1.000e+02 6.600e+01 2.000e+01 9.000e+01 3.290e+01 8.670e-01\n",
      "  2.800e+01 1.000e+00 0.000e+00]\n",
      " [1.300e+01 1.260e+02 9.000e+01 0.000e+00 0.000e+00 4.340e+01 5.830e-01\n",
      "  4.200e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+00 7.900e+01 7.500e+01 3.000e+01 0.000e+00 3.200e+01 3.960e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [7.000e+00 6.200e+01 7.800e+01 0.000e+00 0.000e+00 3.260e+01 3.910e-01\n",
      "  4.100e+01 0.000e+00 0.000e+00]\n",
      " [5.000e+00 9.500e+01 7.200e+01 3.300e+01 0.000e+00 3.770e+01 3.700e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.130e+02 4.400e+01 1.300e+01 0.000e+00 2.240e+01 1.400e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.010e+02 6.500e+01 2.800e+01 0.000e+00 2.460e+01 2.370e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 1.070e+02 6.800e+01 1.900e+01 0.000e+00 2.650e+01 1.650e-01\n",
      "  2.400e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 8.000e+01 5.500e+01 0.000e+00 0.000e+00 1.910e+01 2.580e-01\n",
      "  2.100e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 8.500e+01 6.500e+01 0.000e+00 0.000e+00 3.960e+01 9.300e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 9.600e+01 1.220e+02 0.000e+00 0.000e+00 2.240e+01 2.070e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 8.300e+01 5.800e+01 3.100e+01 1.800e+01 3.430e+01 3.360e-01\n",
      "  2.500e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.710e+02 7.200e+01 3.300e+01 1.350e+02 3.330e+01 1.990e-01\n",
      "  2.400e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+00 8.900e+01 7.600e+01 3.400e+01 3.700e+01 3.120e+01 1.920e-01\n",
      "  2.300e+01 0.000e+00 0.000e+00]\n",
      " [4.000e+00 7.600e+01 6.200e+01 0.000e+00 0.000e+00 3.400e+01 3.910e-01\n",
      "  2.500e+01 0.000e+00 0.000e+00]] \n",
      "\n",
      "FP/FN for MLP\n",
      " [[2.000e+00 1.970e+02 7.000e+01 4.500e+01 5.430e+02 3.050e+01 1.580e-01\n",
      "  5.300e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+01 1.680e+02 7.400e+01 0.000e+00 0.000e+00 3.800e+01 5.370e-01\n",
      "  3.400e+01 1.000e+00 1.000e+00]\n",
      " [7.000e+00 1.070e+02 7.400e+01 0.000e+00 0.000e+00 2.960e+01 2.540e-01\n",
      "  3.100e+01 1.000e+00 0.000e+00]\n",
      " [1.000e+00 1.030e+02 3.000e+01 3.800e+01 8.300e+01 4.330e+01 1.830e-01\n",
      "  3.300e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.580e+02 7.600e+01 3.600e+01 2.450e+02 3.160e+01 8.510e-01\n",
      "  2.800e+01 1.000e+00 1.000e+00]\n",
      " [6.000e+00 9.200e+01 9.200e+01 0.000e+00 0.000e+00 1.990e+01 1.880e-01\n",
      "  2.800e+01 0.000e+00 0.000e+00]\n",
      " [7.000e+00 1.060e+02 9.200e+01 1.800e+01 0.000e+00 2.270e+01 2.350e-01\n",
      "  4.800e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.800e+02 6.600e+01 3.900e+01 0.000e+00 4.200e+01 1.893e+00\n",
      "  2.500e+01 1.000e+00 1.000e+00]\n",
      " [2.000e+00 7.100e+01 7.000e+01 2.700e+01 0.000e+00 2.800e+01 5.860e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [8.000e+00 1.760e+02 9.000e+01 3.400e+01 3.000e+02 3.370e+01 4.670e-01\n",
      "  5.800e+01 1.000e+00 1.000e+00]\n",
      " [5.000e+00 4.400e+01 6.200e+01 0.000e+00 0.000e+00 2.500e+01 5.870e-01\n",
      "  3.600e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 1.410e+02 5.800e+01 3.400e+01 1.280e+02 2.540e+01 6.990e-01\n",
      "  2.400e+01 0.000e+00 0.000e+00]\n",
      " [5.000e+00 9.900e+01 7.400e+01 2.700e+01 0.000e+00 2.900e+01 2.030e-01\n",
      "  3.200e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.090e+02 8.800e+01 3.000e+01 0.000e+00 3.250e+01 8.550e-01\n",
      "  3.800e+01 1.000e+00 0.000e+00]\n",
      " [2.000e+00 1.090e+02 9.200e+01 0.000e+00 0.000e+00 4.270e+01 8.450e-01\n",
      "  5.400e+01 0.000e+00 1.000e+00]\n",
      " [2.000e+00 1.000e+02 6.600e+01 2.000e+01 9.000e+01 3.290e+01 8.670e-01\n",
      "  2.800e+01 1.000e+00 0.000e+00]\n",
      " [1.300e+01 1.260e+02 9.000e+01 0.000e+00 0.000e+00 4.340e+01 5.830e-01\n",
      "  4.200e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+00 7.900e+01 7.500e+01 3.000e+01 0.000e+00 3.200e+01 3.960e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [7.000e+00 6.200e+01 7.800e+01 0.000e+00 0.000e+00 3.260e+01 3.910e-01\n",
      "  4.100e+01 0.000e+00 0.000e+00]\n",
      " [5.000e+00 9.500e+01 7.200e+01 3.300e+01 0.000e+00 3.770e+01 3.700e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.130e+02 4.400e+01 1.300e+01 0.000e+00 2.240e+01 1.400e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [0.000e+00 1.010e+02 6.500e+01 2.800e+01 0.000e+00 2.460e+01 2.370e-01\n",
      "  2.200e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 1.070e+02 6.800e+01 1.900e+01 0.000e+00 2.650e+01 1.650e-01\n",
      "  2.400e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 8.000e+01 5.500e+01 0.000e+00 0.000e+00 1.910e+01 2.580e-01\n",
      "  2.100e+01 0.000e+00 0.000e+00]\n",
      " [2.000e+00 8.500e+01 6.500e+01 0.000e+00 0.000e+00 3.960e+01 9.300e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [1.000e+00 9.600e+01 1.220e+02 0.000e+00 0.000e+00 2.240e+01 2.070e-01\n",
      "  2.700e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 8.300e+01 5.800e+01 3.100e+01 1.800e+01 3.430e+01 3.360e-01\n",
      "  2.500e+01 0.000e+00 0.000e+00]\n",
      " [3.000e+00 1.710e+02 7.200e+01 3.300e+01 1.350e+02 3.330e+01 1.990e-01\n",
      "  2.400e+01 1.000e+00 1.000e+00]\n",
      " [1.000e+00 8.900e+01 7.600e+01 3.400e+01 3.700e+01 3.120e+01 1.920e-01\n",
      "  2.300e+01 0.000e+00 0.000e+00]\n",
      " [4.000e+00 7.600e+01 6.200e+01 0.000e+00 0.000e+00 3.400e+01 3.910e-01\n",
      "  2.500e+01 0.000e+00 0.000e+00]] \n",
      "\n",
      "\n",
      "Gaussian NB analysis with modified parameter var_smoothing=0.01\n",
      "Confusion matrix of GB3: TP = 121 , TN = 32 , FP = 9 , FN = 30\n",
      "The accuracy score test:  0.796875\n",
      "The precision score is:  0.7804878048780488\n",
      "The recall score is:  0.5161290322580645\n",
      "\n",
      "Logistic Regression analysis with modified parameter C: 1e12\n",
      "\n",
      "Confusion matrix: TP = 122 , TN = 36 , FP = 8 , FN = 26\n",
      "The accuracy score is:  0.8229166666666666\n",
      "The precision score is:  0.8181818181818182\n",
      "The recall score is:  0.5806451612903226\n",
      "\n",
      "Multi-layer Perceptron analysis - modified activation to identity (implementing linear bottleneck) - increased # of nodes\n",
      "\n",
      "Confusion matrix of MLP3: TP = 122 , TN = 36 , FP = 8 , FN = 26\n",
      "The accuracy score is:  0.8229166666666666\n",
      "The precision score is:  0.8181818181818182\n",
      "The recall score is:  0.5806451612903226\n",
      "\n",
      "Gaussian NB analysis with modified parameter var_smoothing=0.01\n",
      "Confusion matrix of GB3: TP = 105 , TN = 30 , FP = 17 , FN = 40\n",
      "The accuracy score test:  0.703125\n",
      "The precision score is:  0.6382978723404256\n",
      "The recall score is:  0.42857142857142855\n",
      "\n",
      "Logistic Regression analysis with modified parameter C: 1e12\n",
      "\n",
      "Confusion matrix: TP = 105 , TN = 34 , FP = 17 , FN = 36\n",
      "The accuracy score is:  0.7239583333333334\n",
      "The precision score is:  0.6666666666666666\n",
      "The recall score is:  0.4857142857142857\n",
      "\n",
      "Multi-layer Perceptron analysis - modified activation to identity (implementing linear bottleneck) - increased # of nodes\n",
      "\n",
      "Confusion matrix of MLP3: TP = 105 , TN = 34 , FP = 17 , FN = 36\n",
      "The accuracy score is:  0.7239583333333334\n",
      "The precision score is:  0.6666666666666666\n",
      "The recall score is:  0.4857142857142857\n",
      "\n",
      "Gaussian NB analysis with modified parameter var_smoothing=0.01\n",
      "Confusion matrix of GB3: TP = 107 , TN = 29 , FP = 15 , FN = 41\n",
      "The accuracy score test:  0.7083333333333334\n",
      "The precision score is:  0.6590909090909091\n",
      "The recall score is:  0.4142857142857143\n",
      "\n",
      "Logistic Regression analysis with modified parameter C: 1e12\n",
      "\n",
      "Confusion matrix: TP = 107 , TN = 37 , FP = 15 , FN = 33\n",
      "The accuracy score is:  0.75\n",
      "The precision score is:  0.7115384615384616\n",
      "The recall score is:  0.5285714285714286\n",
      "\n",
      "Multi-layer Perceptron analysis - modified activation to identity (implementing linear bottleneck) - increased # of nodes\n",
      "\n",
      "Confusion matrix of MLP3: TP = 107 , TN = 37 , FP = 15 , FN = 33\n",
      "The accuracy score is:  0.75\n",
      "The precision score is:  0.7115384615384616\n",
      "The recall score is:  0.5285714285714286\n"
     ]
    }
   ],
   "source": [
    "report_ListGB3 = [] # 2D array to store accuracy, precision, recall\n",
    "report_ListLR3 = []\n",
    "report_ListMLP3 = []\n",
    "count = 0\n",
    "\n",
    "# Redoing the cross-validation after chanigng parameters\n",
    "for train_index, test_index in kf.split(x):\n",
    "    x_training, x_testing = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_training, y_testing = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Cross validation for GB after adding prior parameter and var_smoothing\n",
    "    print(\"\\nGaussian NB analysis with modified parameter var_smoothing=0.01\")\n",
    "    gaussNB3 = GaussianNB(var_smoothing=0.01).fit(x_training, y_training)\n",
    "\n",
    "    # Predicting the outcome\n",
    "    y_prediction_GB3 = gaussNB3.predict(x_testing)\n",
    "\n",
    "    # confusion matrix\n",
    "    conmGB3 = confusion_matrix(y_testing, y_prediction_GB3)\n",
    "    print(\"Confusion matrix of GB3: TP =\", conmGB3[0,0], \", TN =\", conmGB3[1,1], \", FP =\", conmGB3[0,1], \", FN =\", conmGB3[1,0])\n",
    "    print(\"The accuracy score test: \", accuracy_score(y_testing, y_prediction_GB3))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_GB3))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_GB3))\n",
    "    cl_report_GB3 =classification_report(y_testing, y_prediction_GB3)\n",
    "    #print(\"Classification report below:\", count)\n",
    "    #print(cl_report_GB2)\n",
    "    class_report_GB3 = [accuracy_score(y_testing, y_prediction_GB3), precision_score(y_testing, y_prediction_GB3), recall_score(y_testing, y_prediction_GB2)]\n",
    "    report_ListGB3.append(class_report_GB3)\n",
    "\n",
    "    # Model 3 for Logistic Regression: Parameter changed : C : inverse of regularization\n",
    "    # Training and testing LogisticRegression with modifief parameter: C = 1e12 to predict on test set better and reduce overfitting\n",
    "    print(\"\\nLogistic Regression analysis with modified parameter C: 1e12\\n\")\n",
    "    logistic_reg3 = LogisticRegression(penalty='l2', fit_intercept = True, C=1e12, max_iter=1000).fit(x_training, y_training)\n",
    "    y_prediction_LR3 = logistic_reg3.predict(x_testing)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conmLR3 = confusion_matrix(y_testing, y_prediction_LR3)\n",
    "    print(\"Confusion matrix: TP =\", conmLR3[0,0], \", TN =\", conmLR3[1,1], \", FP =\", conmLR3[0,1], \", FN =\", conmLR3[1,0])\n",
    "    print(\"The accuracy score is: \", accuracy_score(y_testing, y_prediction_LR3))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_LR3))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_LR3))\n",
    "\n",
    "    cl_report_LR3 = classification_report(y_testing, y_prediction_LR3, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    #print(\"Classification report below:\", count)\n",
    "    #print(cl_report_LR3)\n",
    "    class_report_LR3 = [accuracy_score(y_testing, y_prediction_LR3), precision_score(y_testing, y_prediction_LR3), recall_score(y_testing, y_prediction_LR3)]\n",
    "    report_ListLR3.append(class_report_LR3)\n",
    "   \n",
    "   \n",
    "    # Model 3 for MLP classification with 1 hidden layer, Changing parameter type solver to sgd\n",
    "\n",
    "    print(\"\\nMulti-layer Perceptron analysis - modified activation to identity (implementing linear bottleneck) - increased # of nodes\\n\")\n",
    "    mlp3 = MLPClassifier(solver=\"lbfgs\", max_iter=3000, batch_size='auto', activation=\"identity\", hidden_layer_sizes=(13), alpha=0.0001, learning_rate_init=0.001, random_state=2)\n",
    "    mlp3.fit(x_training, y_training)\n",
    "\n",
    "    y_prediction_MLP3 = mlp3.predict(x_testing)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conmMLP3 = confusion_matrix(y_testing, y_prediction_MLP3)\n",
    "    print(\"Confusion matrix of MLP3: TP =\", conmMLP3[0,0], \", TN =\", conmMLP3[1,1], \", FP =\", conmMLP3[0,1], \", FN =\", conmMLP3[1,0])\n",
    "    print(\"The accuracy score is: \", accuracy_score(y_testing, y_prediction_MLP3))\n",
    "    print(\"The precision score is: \", precision_score(y_testing, y_prediction_MLP3))\n",
    "    print(\"The recall score is: \", recall_score(y_testing, y_prediction_MLP3))\n",
    "\n",
    "    cl_report_MLP3 = classification_report(y_testing, y_prediction_MLP3, target_names=[\"No diabetes\", \"Diabetes\"])\n",
    "    #print(\"Classification report below\", count)\n",
    "    #print(cl_report_MLP3)\n",
    "    class_report_MLP3 = [accuracy_score(y_testing, y_prediction_MLP3), precision_score(y_testing, y_prediction_MLP3), recall_score(y_testing, y_prediction_MLP3)]\n",
    "    report_ListMLP3.append(class_report_MLP3)\n",
    "\n",
    "    count+=1\n",
    "    if(count==1):\n",
    "        headers = dataFrame.columns\n",
    "        print(\"The last two columns are: Actual Diagnosis, Predicted Diagnosis\")\n",
    "        print(\"FP/FN for GB\\n\", bn1, \"\\n\")\n",
    "        an1 = np.column_stack((x_testing, y_testing, y_prediction_GB3))\n",
    "        bn1 = an1[:30,:]\n",
    "        print(\"FP/FN for LR\\n\", bn2, \"\\n\")\n",
    "        an2 = np.column_stack((x_testing, y_testing, y_prediction_LR3))\n",
    "        bn2 = an2[:30,:]\n",
    "        print(\"FP/FN for MLP\\n\", bn, \"\\n\")\n",
    "        an = np.column_stack((x_testing, y_testing, y_prediction_MLP3))\n",
    "        bn = an[:30,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c3063",
   "metadata": {},
   "source": [
    "#### Calculating Average again:\n",
    "Now doing an average of all the cross-validation 4 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b02e69e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy, precision, recall after cross validation of 3rd GB model: [0.75260417 0.71738581 0.48167854]\n",
      "Average accuracy, precision, recall after cross validation of 3rd LR model: [0.77473958 0.73243007 0.56539939]\n",
      "Average accuracy, precision, recall after cross validation of 3rd MLP model: [0.77604167 0.73553741 0.56539939]\n"
     ]
    }
   ],
   "source": [
    "# Now doing an average of the two models, since there is only priors that could change and there is no need to do variance \n",
    "# since there is no zero probability.\n",
    "\n",
    "averaged_reportGB3 = np.array(report_ListGB3).mean(axis=0)\n",
    "print(\"Average accuracy, precision, recall after cross validation of 3rd GB model:\", averaged_reportGB3)\n",
    "\n",
    "averaged_reportLR3 = np.array(report_ListLR3).mean(axis=0)\n",
    "print(\"Average accuracy, precision, recall after cross validation of 3rd LR model:\", averaged_reportLR3)\n",
    "#average_allGB = np.array(averaged_reportGB, averaged_reportGB2)\n",
    "averaged_reportMLP3 = np.array(report_ListMLP3).mean(axis=0)\n",
    "print(\"Average accuracy, precision, recall after cross validation of 3rd MLP model:\", averaged_reportMLP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac292818",
   "metadata": {},
   "source": [
    "### Step 10 - Analyzing the results\n",
    "\n",
    "### Comparing the pecision/recall measures of the 9 results quantitatively.\n",
    " <br>\n",
    "\n",
    "The confusion matrix of each cross-validation section and all types of models was printed. The in-built sklearn functions of accuracy, precision and recall were used, and their averages were taken for comparison with each other.\n",
    "\n",
    "#### Gaussian Naive Bayes:\n",
    "As can be seen from the data, the best accuracy and precision of Gaussian Naive Bayes models was provided by the third Gaussian Naive Bayes model with variance smoothing set to 0.01. The average accuracy for that is 0.7526, precision is 0.7174, but the recall is low at 0.4817. While the precision and accuracy increased, the recall decreased. This means that the algorithm detects the true positives and returns the right predictions compared to the other two Naive Bayes models with lower precision. The variance of each attribute was taken into consideration, which increased the accuracy and precision of the model. This is because there are certain 0 values that skews the result. Hence, taking that into consideration allowed for a better prediction. \n",
    "\n",
    "#### Logistic Regression:\n",
    "As can be seen from the data, the best accuracy and precision of Logistic Regression models was provided by the third Logistic Regression model with modified C(inverse of regularization strength). The average accuracy for that is 0.7747, precision is 0.7324 and lowest precision at 0.5654. While the precision and accuracy increased, the recall decreased. This means that the algorithm fits the model better since the modified C float defines how much the algorithm should be regularized. The smaller the number, the more it is regularized. Therefore, decreasing regularization prevents the training model from overfitting. Therefore, in this case, the model gave us better accuracy, precision and lower recall. The higher precision means that the model returns more right predictions compared to the other two regression models with lower precision.\n",
    "\n",
    "#### Multi-Layer Perceptron:\n",
    "The best accuracy and precision of Multi-Layer Perceptron models was provided by the third Multi-Layer Perceptron classification model with modified identity activation function for the hidden layer. This model has 'identity' instead of the 'relu' default function, and the hidden layer has 13 nodes. The average accuracy for this model is 0.7760, precision is 0.7355, and the highest recall at 0.5654. In this case, unlike the other two types of models, MLP model's recall increased along with precision. The higher recall shows that with the third MLP model, greater proportion of positive diagnosis was correct compared to other two MLP models. When playing with changing parameters, it was noticed that changing the activation function had a higher impact on the confusion matrix than increasing the nodes in the hidden layers. The activation identity function yields a linear transformation and therefore it fits better to the model. \n",
    "\n",
    "Overall, out of all types of models, the best accuracy and precision was provided by the third MLP classification model using identity activation function with 1 hidden layer containing 13 nodes. The lowest recall was given by the second Gaussian Naive Bayes model with prior probabilities, meaning it predicted a greater proportion of actual positives accurately, and lower false negatives.\n",
    "\n",
    "Overall, in this case when predicting the diagnosis of Diabetes, it is better to have higher recall since that would mean that at least there will be lower number of false negatives. It is better that people get a positive prediction even if they do not have diabetes than the people with diabetes getting incorrect prediction since they are at risk of not getting proper treatment. The patients who receive an incorrect prediction can do an actual test to find out if they have the diabetes or not.\n",
    "\n",
    "#### Examples for the above analysis:\n",
    "For third model of Gaussian Naive Bayes, there are much more false negatives than false positives, for instance at index 17 has a false negative when doing a cross validation in second iteration. This is again because of high precision.\n",
    "In addition, there is a false negative for for index 17 in Logistic regression and Naive Bayes on the same section. This shows that it is becuase of the missing values for skin thickness and insulin in point 17.  \n",
    "For example, the third Naive Bayes gives the incorrect prediction, which is a false negative for data point at index 17 (starting from 0). It is supposed to diagnose that the patient has diabetes, but it predicts no diabetes. This is because the precision is high. While index 67 gives a false positive. The result has been printed above in the second iteration of the third models for all. I did not keep the other printed runs to avoid making this report long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab85af",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. https://www.geeksforgeeks.org/how-to-get-column-names-in-pandas-dataframe/\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold.get_n_splits\n",
    "4. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict\n",
    "5. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay.from_predictions\n",
    "6. https://holypython.com/nbc/naive-bayes-classifier-optimization-parameters/\n",
    "7. https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "8. https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "Choosing continuous or discrete variables:<br>\n",
    "9. https://medium.com/@christopherfielding/na%C3%AFve-bayes-classification-for-discrete-and-continuous-variables-cb1103155488 <br>\n",
    "For layers: <br>\n",
    "10. https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3 <br>\n",
    "Dataset: <br>\n",
    "11: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database <br>\n",
    "12. https://www.kaggle.com/code/logeshk/pima-indians-diabetes-logistic-regression\n",
    "\n",
    "The references mentioned in the Project description."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
